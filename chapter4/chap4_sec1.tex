In this chapter, the focus lies on showcasing the setup process, resource utilization, dataset selection,
and experimental comparison between two methods: sequential encounter encoding and the standard method for solving the itemset mining problem.
The chapter will provide detailed instructions on how to install the necessary tools and libraries, specify the hardware and software resources utilized,
describe the datasets employed for experimentation, and present the results of the comparative analysis between the sequential encounter encoding method and the standard method.
\section{Setup and Datasets}
\subsection{Setup}

The experiments were conducted on a machine with the following specifications:
\begin{itemize}
    \item Processor: Intel Core i7-7700HQ CPU @ 2.80GHz
    \item Memory: 16GB DDR4 RAM
    \item Operating System: Ubuntu 20.04 LTS
\end{itemize}
For the setup, the experiments are conducted using Python 3.8 as main programming language,
and the following libraries are used such as NumPy, Pandas, and Matplotlib for data manipulation and visualization.

The SAT Solver used in the experiments is Kissat.
Kissat is a "keep it simple and clean bare metal SAT solver"
written in C. It is a port of CaDiCaL back to C with improved data structures,
better scheduling of inprocessing and optimized algorithms and implementation.
The source code for Kissat can be found at \url{https://github.com/arminbiere/kissat}.

After cloning the repository, the solver can be built and run by executing a command in the command line using the subprocess module in Python.
This allows for seamless integration of the solver into existing Python scripts or workflows.
The subprocess module provides a way to spawn new processes, connect to their input/output/error pipes, and obtain their return codes.
By utilizing this module, the solver can be invoked and its output can be captured and processed programmatically.
This provides flexibility and automation in running the solver and analyzing its results.

But Kissat is find only the first solution, so we need to modify the source code to find all solutions.
By adding solved result to the list and continue to find the next solution until there is no solution left.

\subsection{Generated Datasets}
To compare the performance of the sequential encounter encoding method with the standard method.
In this repository, it provides a synthetic dataset generator that can generate a dataset with a specified number of transactions, items and minimun support.
The generator is written in Python and can be found in the \texttt{generator.py} file in folder \texttt{input}.
\begin{flushleft}
    \textbf{Parameters}: The generator takes the following parameters:
\end{flushleft}
\begin{itemize}
    \item \texttt{transactions}: the number of transactions in the dataset
    \item \texttt{items}: the number of items in the dataset
    \item \texttt{min\_support}: the minimum support threshold for frequent itemsets
    \item \texttt{output\_file}: the path of the output file
\end{itemize}

\begin{flushleft}
    \textbf{Output}: The generator outputs a dataset in the annotated transaction format.
    Each line represents a transaction, with items separated by spaces.
\end{flushleft}

\begin{flushleft}
    \textbf{How to run}: Dataset can be generated by manual script or during benchmarking.
\end{flushleft}

\subsection{Real-World Datasets}
The datasets used in the experiments are coming from FIMI\footnote{\url{http://fimi.uantwerpen.be/data/}} and CP4IM\footnote{\url{https://dtai.cs.kuleuven.be/CP4IM/datasets/}}.
The characteristics of the considered datasets are given in Table \ref{tab:datasets}.
% draw table
\begin{table}[H]
    \centering
    \begin{tabular}{|l|c|c|c|c|}
        \hline
        Dataset       & Transactions & Items \\
        \hline
        zoo-1         & 101          & 36    \\
        primary-tumor & 336          & 32    \\
        vote          & 435          & 48    \\
        soybean       & 630          & 50    \\
        chess         & 3196         & 75    \\
        mushroom      & 8124         & 119   \\
        \hline
    \end{tabular}
    \caption{Characteristics of the considered datasets}
    \label{tab:datasets}
\end{table}

% explain the FIMI and CP4IM datasets
The FIMI datasets are commonly utilized in the data mining community to assess the performance of frequent itemset mining algorithms.
These datasets are designed to evaluate the ability of algorithms to discover frequent itemsets efficiently.
On the other hand, the CP4IM datasets are specifically created for evaluating constraint programming-based itemset mining algorithms.
These datasets are formatted in ARFF, which is a file format commonly used by the Weka data mining software.
To ensure compatibility with both the sequential encounter encoding method and the standard method, the datasets have been preprocessed and converted into a suitable format.

\textbf{Format}: The datasets are in annotated transaction format with labels: every line is one transaction.
A transaction is a space-separated list of item identifiers (offset 0), the last item is either 1 or 0 and represents the class label.
The meaning of every label is given in the header of the file: $@<nr>: ...$ lines describe item number $<nr>, @class: ...$ describes the two classes.
To parse the files correctly, all lines starting with @, with \% and empty lines should be ignored. (the format is a combination of the FIMI format with annotations like the ARFF format).

\textbf{Preprocessing}: The datasets are preprocessed to remove any unnecessary information and ensure that they are in the correct format.
\begin{itemize}
    \item Remove all tag lines starting with @, with \% and empty lines.
    \item Remove class labels from the transactions.
    \item Run the script \texttt{convert\_real\_world.py} to preprocess the datasets into readable format.
    \item The preprocessed datasets are saved in the \texttt{input} folder.
\end{itemize}
